preprocessing on output/mnist/1212.model
adv_loss=4.827976226806641e-06 adv_loss_mm=4.708766937255859e-06 inp_loss=4.27380895614624 gap=-1.518417445762958
preprocessing on output/mnist/1459.model
adv_loss=1.430511474609375e-06 adv_loss_mm=2.6226043701171875e-06 inp_loss=4.129623889923096 gap=-1.0982149079709458
preprocessing on output/mnist/1461.model
adv_loss=4.649162292480469e-06 adv_loss_mm=3.5762786865234375e-06 inp_loss=3.348453998565674 gap=-2.074641341689596
preprocessing on output/mnist/2641.model
adv_loss=2.175569534301758e-06 adv_loss_mm=9.834766387939453e-07 inp_loss=3.1373472213745117 gap=-2.2001739081808616
preprocessing on output/mnist/2782.model
adv_loss=3.4868717193603516e-06 adv_loss_mm=4.559755325317383e-06 inp_loss=3.4540209770202637 gap=-0.03957562719990185
preprocessing on output/mnist/3860.model
adv_loss=2.0265579223632812e-06 adv_loss_mm=3.337860107421875e-06 inp_loss=3.865034580230713 gap=-3.673972438462156
preprocessing on output/mnist/3966.model
adv_loss=4.172325134277344e-07 adv_loss_mm=8.940696716308594e-07 inp_loss=2.7201309204101562 gap=-5.0190233333916646e-08
preprocessing on output/mnist/41.model
adv_loss=1.6093254089355469e-06 adv_loss_mm=3.159046173095703e-06 inp_loss=2.527252674102783 gap=-1.925067271937266e-07
preprocessing on output/mnist/4798.model
adv_loss=2.086162567138672e-06 adv_loss_mm=1.7285346984863281e-06 inp_loss=3.2024991512298584 gap=-0.8083748030881506
preprocessing on output/mnist/6327.model
adv_loss=2.0563602447509766e-06 adv_loss_mm=1.1026859283447266e-06 inp_loss=3.296889305114746 gap=-2.081276688754929e-07
preprocessing on output/mnist/6576.model
adv_loss=1.3113021850585938e-06 adv_loss_mm=2.5033950805664062e-06 inp_loss=2.962254524230957 gap=-1.6094605991540831
preprocessing on output/mnist/6791.model
adv_loss=2.6673078536987305e-06 adv_loss_mm=2.9653310775756836e-06 inp_loss=3.8554153442382812 gap=-0.09270705339313404
preprocessing on output/mnist/6929.model
adv_loss=2.5480985641479492e-06 adv_loss_mm=1.1175870895385742e-06 inp_loss=4.787814140319824 gap=-1.845350300828507
preprocessing on output/mnist/7186.model
adv_loss=3.337860107421875e-06 adv_loss_mm=2.2649765014648438e-06 inp_loss=3.554490089416504 gap=-7.865661580616019e-07
preprocessing on output/mnist/850.model
adv_loss=2.2649765014648438e-06 adv_loss_mm=8.344650268554688e-07 inp_loss=3.7251996994018555 gap=-2.680255589217079e-06
preprocessing on output/mnist/8571.model
adv_loss=1.430511474609375e-06 adv_loss_mm=2.384185791015625e-06 inp_loss=3.511122703552246 gap=-7.943013895155949e-07
preprocessing on output/mnist/890.model
adv_loss=1.430511474609375e-06 adv_loss_mm=1.5497207641601562e-06 inp_loss=2.5032901763916016 gap=-1.022442080828036e-07
preprocessing on output/mnist/9285.model
adv_loss=1.1920928955078125e-06 adv_loss_mm=2.384185791015625e-06 inp_loss=2.972048044204712 gap=-0.48362446971144474
working on output/mnist/850.model with loss=8.34465e-07
#0: loss: 8.34465e-07 -> 7.15256e-07 diff=1.19e-07,1.19e-07 Linf=1.863e-07
#0: loss: 7.15256e-07 -> 5.96046e-07 diff=2.38e-07,2.38e-07 Linf=1.863e-07
#0: loss: 5.96046e-07 -> -1.19209e-07 diff=9.54e-07,9.54e-07 Linf=1.863e-07
#1: loss: -1.19209e-07 -> -2.38419e-07 diff=1.07e-06,1.07e-06 Linf=2.086e-07
#3: loss: -2.38419e-07 -> -3.57628e-07 diff=1.19e-06,1.19e-06 Linf=2.086e-07
#10: loss: -3.57628e-07 -> -4.76837e-07 diff=1.31e-06,1.31e-06 Linf=2.086e-07
conv [-7.82 -8.05 -6.81 -4.62  1.16 -5.05 -7.89 -1.23 -4.46  1.16] l=9 cw=3.10e-06
mm   [-7.82 -8.05 -6.81 -4.62  1.16 -5.05 -7.89 -1.23 -4.46  1.16] l=4 cw=-4.77e-07
test acc: 98.57%
working on output/mnist/3966.model with loss=8.940697e-07
#0: loss: 8.94070e-07 -> 1.78814e-07 diff=7.15e-07,7.15e-07 Linf=1.795e-07
#0: loss: 1.78814e-07 -> -2.98023e-07 diff=1.19e-06,1.19e-06 Linf=2.086e-07
#10: loss: -2.98023e-07 -> -4.17233e-07 diff=1.31e-06,1.31e-06 Linf=2.086e-07
#14: loss: -4.17233e-07 -> -5.36442e-07 diff=1.43e-06,1.43e-06 Linf=2.086e-07
#309: loss: -5.36442e-07 -> -6.55651e-07 diff=1.55e-06,1.55e-06 Linf=2.086e-07
#346: loss: -6.55651e-07 -> -7.74860e-07 diff=1.67e-06,1.67e-06 Linf=2.086e-07
#469: loss: -7.74860e-07 -> -8.94070e-07 diff=1.79e-06,1.79e-06 Linf=3.337e-07
#471: loss: -8.94070e-07 -> -1.01328e-06 diff=1.91e-06,1.91e-06 Linf=3.387e-07
#477: loss: -1.01328e-06 -> -1.13249e-06 diff=2.03e-06,2.03e-06 Linf=3.387e-07
#514: loss: -1.13249e-06 -> -1.25170e-06 diff=2.15e-06,2.15e-06 Linf=3.387e-07
conv [-9.52 -4.02 -5.99 -3.45 -0.2  -5.1  -8.7  -3.2  -2.61 -0.2 ] l=9 cw=1.49e-06
mm   [-9.52 -4.02 -5.99 -3.45 -0.2  -5.1  -8.7  -3.2  -2.61 -0.2 ] l=4 cw=-1.25e-06
test acc: 98.57%
working on output/mnist/2641.model with loss=9.834766e-07
#0: loss: 9.83477e-07 -> 8.64267e-07 diff=1.19e-07,1.19e-07 Linf=2.384e-07
#2: loss: 8.64267e-07 -> 7.45058e-07 diff=2.38e-07,2.38e-07 Linf=2.384e-07
#2: loss: 7.45058e-07 -> 5.06639e-07 diff=4.77e-07,4.77e-07 Linf=2.384e-07
#3: loss: 5.06639e-07 -> 3.87430e-07 diff=5.96e-07,5.96e-07 Linf=2.384e-07
#4: loss: 3.87430e-07 -> -8.94070e-08 diff=1.07e-06,1.07e-06 Linf=2.384e-07
#60: loss: -8.94070e-08 -> -2.08616e-07 diff=1.19e-06,1.19e-06 Linf=2.384e-07
#528: loss: -2.08616e-07 -> -4.47035e-07 diff=1.43e-06,1.43e-06 Linf=2.384e-07
conv [-6.83 -4.77 -4.53 -5.57 -3.72 -0.23 -0.23 -7.11 -1.89 -5.37] l=6 cw=2.29e-06
mm   [-6.83 -4.77 -4.53 -5.57 -3.72 -0.23 -0.23 -7.11 -1.89 -5.37] l=5 cw=-4.47e-07
test acc: 98.40%
working on output/mnist/6327.model with loss=1.1026859e-06
#0: loss: 1.10269e-06 -> 8.64267e-07 diff=2.38e-07,2.38e-07 Linf=2.384e-07
#2: loss: 8.64267e-07 -> 6.25849e-07 diff=4.77e-07,4.77e-07 Linf=2.384e-07
#2: loss: 6.25849e-07 -> 3.87430e-07 diff=7.15e-07,7.15e-07 Linf=2.384e-07
#23: loss: 3.87430e-07 -> 1.49012e-07 diff=9.54e-07,9.54e-07 Linf=2.384e-07
#75: loss: 1.49012e-07 -> -8.94070e-08 diff=1.19e-06,1.19e-06 Linf=2.384e-07
conv [-3.97 -4.93 -4.93 -4.77 -5.1  -0.31 -2.56 -7.34 -0.31 -5.6 ] l=8 cw=1.82e-06
mm   [-3.97 -4.93 -4.93 -4.77 -5.1  -0.31 -2.56 -7.34 -0.31 -5.6 ] l=5 cw=-8.94e-08
test acc: 98.57%
working on output/mnist/6929.model with loss=1.1175871e-06
#3: loss: 1.11759e-06 -> 8.79169e-07 diff=2.38e-07,2.38e-07 Linf=1.639e-07
#33: loss: 8.79169e-07 -> 6.40750e-07 diff=4.77e-07,4.77e-07 Linf=2.384e-07
#151: loss: 6.40750e-07 -> 4.02331e-07 diff=7.15e-07,7.15e-07 Linf=2.682e-07
#237: loss: 4.02331e-07 -> 1.63913e-07 diff=9.54e-07,9.54e-07 Linf=2.682e-07
#287: loss: 1.63913e-07 -> -7.45058e-08 diff=1.19e-06,1.19e-06 Linf=2.682e-07
#381: loss: -7.45058e-08 -> -3.12924e-07 diff=1.43e-06,1.43e-06 Linf=2.682e-07
conv [-5.97 -6.51 -5.8  -5.91  0.19 -6.92 -4.68 -5.63  0.19 -3.06] l=4 cw=3.02e-06
mm   [-5.97 -6.51 -5.8  -5.91  0.19 -6.92 -4.68 -5.63  0.19 -3.06] l=8 cw=-3.13e-07
test acc: 98.14%
verification of original model failed: {'ObjectiveBound': -1.012973834979558e-06,
 'ObjectiveValue': -1.012973834979558e-06,
 'PredictedIndex': 5,
 'SolveStatus': 'Optimal',
 'SolveTime': 22.17134118080139,
 'TargetIndexes': array([ 1,  2,  3,  4,  6,  7,  8,  9, 10]),
 'TighteningApproach': array([[108],
       [112]], dtype=uint16),
 'TotalTime': 32.608867321,
 'robust': False,
 'status_known': True}
retrying with new gap -1.845349907875061 ...
conv [-5.97 -6.51 -5.8  -5.91  0.19 -6.92 -4.68 -5.63  0.19 -3.06] l=4 cw=3.38e-06
mm   [-5.97 -6.51 -5.8  -5.91  0.19 -6.92 -4.68 -5.63  0.19 -3.06] l=4 cw=4.47e-08
test acc: 98.14%
working on output/mnist/890.model with loss=1.5497208e-06
#0: loss: 1.54972e-06 -> 9.53674e-07 diff=5.96e-07,5.96e-07 Linf=1.799e-07
#0: loss: 9.53674e-07 -> 7.15256e-07 diff=8.34e-07,8.34e-07 Linf=1.856e-07
#1: loss: 7.15256e-07 -> 5.96046e-07 diff=9.54e-07,9.54e-07 Linf=3.095e-07
#2: loss: 5.96046e-07 -> 4.76837e-07 diff=1.07e-06,1.07e-06 Linf=3.095e-07
#32: loss: 4.76837e-07 -> 3.57628e-07 diff=1.19e-06,1.19e-06 Linf=3.095e-07
#33: loss: 3.57628e-07 -> 2.38419e-07 diff=1.31e-06,1.31e-06 Linf=3.095e-07
#51: loss: 2.38419e-07 -> 1.19209e-07 diff=1.43e-06,1.43e-06 Linf=3.095e-07
working on output/mnist/4798.model with loss=1.7285347e-06
#0: loss: 1.72853e-06 -> 1.49012e-06 diff=2.38e-07,2.38e-07 Linf=1.937e-07
#0: loss: 1.49012e-06 -> 1.25170e-06 diff=4.77e-07,4.77e-07 Linf=1.937e-07
#0: loss: 1.25170e-06 -> 1.13249e-06 diff=5.96e-07,5.96e-07 Linf=2.384e-07
#0: loss: 1.13249e-06 -> 8.94070e-07 diff=8.34e-07,8.34e-07 Linf=2.384e-07
#0: loss: 8.94070e-07 -> 6.55651e-07 diff=1.07e-06,1.07e-06 Linf=2.384e-07
#1: loss: 6.55651e-07 -> 4.17233e-07 diff=1.31e-06,1.31e-06 Linf=2.384e-07
#392: loss: 4.17233e-07 -> 2.98023e-07 diff=1.43e-06,1.43e-06 Linf=2.384e-07
working on output/mnist/7186.model with loss=2.2649765e-06
#0: loss: 2.26498e-06 -> 2.02656e-06 diff=2.38e-07,2.38e-07 Linf=1.797e-07
#0: loss: 2.02656e-06 -> 1.78814e-06 diff=4.77e-07,4.77e-07 Linf=1.830e-07
#1: loss: 1.78814e-06 -> 1.66893e-06 diff=5.96e-07,5.96e-07 Linf=1.830e-07
#2: loss: 1.66893e-06 -> 1.54972e-06 diff=7.15e-07,7.15e-07 Linf=2.359e-07
#3: loss: 1.54972e-06 -> 1.31130e-06 diff=9.54e-07,9.54e-07 Linf=3.082e-07
#30: loss: 1.31130e-06 -> 9.53674e-07 diff=1.31e-06,1.31e-06 Linf=3.082e-07
#72: loss: 9.53674e-07 -> 8.34465e-07 diff=1.43e-06,1.43e-06 Linf=3.817e-07
working on output/mnist/8571.model with loss=2.3841858e-06
#0: loss: 2.38419e-06 -> 2.14577e-06 diff=2.38e-07,2.38e-07 Linf=1.416e-07
#1: loss: 2.14577e-06 -> 1.90735e-06 diff=4.77e-07,4.77e-07 Linf=2.384e-07
#1: loss: 1.90735e-06 -> 1.43051e-06 diff=9.54e-07,9.54e-07 Linf=2.384e-07
#10: loss: 1.43051e-06 -> 9.53674e-07 diff=1.43e-06,1.43e-06 Linf=2.384e-07
#106: loss: 9.53674e-07 -> 7.15256e-07 diff=1.67e-06,1.67e-06 Linf=2.988e-07
#112: loss: 7.15256e-07 -> 4.76837e-07 diff=1.91e-06,1.91e-06 Linf=2.988e-07
#167: loss: 4.76837e-07 -> 2.38419e-07 diff=2.15e-06,2.15e-06 Linf=2.988e-07
#339: loss: 2.38419e-07 -> 0.00000e+00 diff=2.38e-06,2.38e-06 Linf=3.832e-07
working on output/mnist/9285.model with loss=2.3841858e-06
#0: loss: 2.38419e-06 -> 2.26498e-06 diff=1.19e-07,1.19e-07 Linf=1.919e-07
#0: loss: 2.26498e-06 -> 1.90735e-06 diff=4.77e-07,4.77e-07 Linf=1.919e-07
#0: loss: 1.90735e-06 -> 1.78814e-06 diff=5.96e-07,5.96e-07 Linf=1.919e-07
#1: loss: 1.78814e-06 -> 1.54972e-06 diff=8.34e-07,8.34e-07 Linf=2.012e-07
#1: loss: 1.54972e-06 -> 1.31130e-06 diff=1.07e-06,1.07e-06 Linf=2.012e-07
#2: loss: 1.31130e-06 -> 1.19209e-06 diff=1.19e-06,1.19e-06 Linf=2.012e-07
#2: loss: 1.19209e-06 -> 1.07288e-06 diff=1.31e-06,1.31e-06 Linf=2.012e-07
#103: loss: 1.07288e-06 -> 8.34465e-07 diff=1.55e-06,1.55e-06 Linf=3.576e-07
#135: loss: 8.34465e-07 -> 7.15256e-07 diff=1.67e-06,1.67e-06 Linf=3.576e-07
#141: loss: 7.15256e-07 -> 5.96046e-07 diff=1.79e-06,1.79e-06 Linf=3.576e-07
working on output/mnist/6576.model with loss=2.503395e-06
#0: loss: 2.50340e-06 -> 2.26498e-06 diff=2.38e-07,2.38e-07 Linf=1.714e-07
#0: loss: 2.26498e-06 -> 2.14577e-06 diff=3.58e-07,3.58e-07 Linf=1.863e-07
#0: loss: 2.14577e-06 -> 2.02656e-06 diff=4.77e-07,4.77e-07 Linf=2.012e-07
#12: loss: 2.02656e-06 -> 1.78814e-06 diff=7.15e-07,7.15e-07 Linf=2.389e-07
#31: loss: 1.78814e-06 -> 1.66893e-06 diff=8.34e-07,8.34e-07 Linf=2.389e-07
#33: loss: 1.66893e-06 -> 1.43051e-06 diff=1.07e-06,1.07e-06 Linf=2.389e-07
#37: loss: 1.43051e-06 -> 1.19209e-06 diff=1.31e-06,1.31e-06 Linf=2.389e-07
#90: loss: 1.19209e-06 -> 9.53674e-07 diff=1.55e-06,1.55e-06 Linf=2.389e-07
#103: loss: 9.53674e-07 -> 8.34465e-07 diff=1.67e-06,1.67e-06 Linf=3.576e-07
#332: loss: 8.34465e-07 -> 7.15256e-07 diff=1.79e-06,1.79e-06 Linf=3.576e-07
working on output/mnist/1459.model with loss=2.6226044e-06
#0: loss: 2.62260e-06 -> 2.50340e-06 diff=1.19e-07,1.19e-07 Linf=2.384e-07
#0: loss: 2.50340e-06 -> 2.38419e-06 diff=2.38e-07,2.38e-07 Linf=2.384e-07
#0: loss: 2.38419e-06 -> 2.26498e-06 diff=3.58e-07,3.58e-07 Linf=2.384e-07
#1: loss: 2.26498e-06 -> 1.78814e-06 diff=8.34e-07,8.34e-07 Linf=2.384e-07
#32: loss: 1.78814e-06 -> 1.66893e-06 diff=9.54e-07,9.54e-07 Linf=2.384e-07
#49: loss: 1.66893e-06 -> 1.54972e-06 diff=1.07e-06,1.07e-06 Linf=2.384e-07
#155: loss: 1.54972e-06 -> 1.43051e-06 diff=1.19e-06,1.19e-06 Linf=2.980e-07
#158: loss: 1.43051e-06 -> 1.37091e-06 diff=1.25e-06,1.25e-06 Linf=2.980e-07
working on output/mnist/6791.model with loss=2.965331e-06
#0: loss: 2.96533e-06 -> 2.84612e-06 diff=1.19e-07,1.19e-07 Linf=1.937e-07
#0: loss: 2.84612e-06 -> 2.60770e-06 diff=3.58e-07,3.58e-07 Linf=1.937e-07
#0: loss: 2.60770e-06 -> 2.48849e-06 diff=4.77e-07,4.77e-07 Linf=1.962e-07
#1: loss: 2.48849e-06 -> 2.13087e-06 diff=8.34e-07,8.34e-07 Linf=2.530e-07
#1: loss: 2.13087e-06 -> 1.77324e-06 diff=1.19e-06,1.19e-06 Linf=2.766e-07
#2: loss: 1.77324e-06 -> 1.59442e-06 diff=1.37e-06,1.37e-06 Linf=2.766e-07
#2: loss: 1.59442e-06 -> 1.35601e-06 diff=1.61e-06,1.61e-06 Linf=2.766e-07
#3: loss: 1.35601e-06 -> 1.23680e-06 diff=1.73e-06,1.73e-06 Linf=3.059e-07
#3: loss: 1.23680e-06 -> 9.98378e-07 diff=1.97e-06,1.97e-06 Linf=3.059e-07
#13: loss: 9.98378e-07 -> 9.38773e-07 diff=2.03e-06,2.03e-06 Linf=3.059e-07
#16: loss: 9.38773e-07 -> 8.19564e-07 diff=2.15e-06,2.15e-06 Linf=3.059e-07
#118: loss: 8.19564e-07 -> 7.00355e-07 diff=2.26e-06,2.26e-06 Linf=3.059e-07
working on output/mnist/41.model with loss=3.1590462e-06
#0: loss: 3.15905e-06 -> 2.68221e-06 diff=4.77e-07,4.77e-07 Linf=1.961e-07
#0: loss: 2.68221e-06 -> 2.56300e-06 diff=5.96e-07,5.96e-07 Linf=1.961e-07
#0: loss: 2.56300e-06 -> 2.20537e-06 diff=9.54e-07,9.54e-07 Linf=1.978e-07
#1: loss: 2.20537e-06 -> 2.08616e-06 diff=1.07e-06,1.07e-06 Linf=1.978e-07
#9: loss: 2.08616e-06 -> 1.84774e-06 diff=1.31e-06,1.31e-06 Linf=1.978e-07
#10: loss: 1.84774e-06 -> 1.72853e-06 diff=1.43e-06,1.43e-06 Linf=1.978e-07
#31: loss: 1.72853e-06 -> 1.60933e-06 diff=1.55e-06,1.55e-06 Linf=2.980e-07
#40: loss: 1.60933e-06 -> 1.37091e-06 diff=1.79e-06,1.79e-06 Linf=2.980e-07
#89: loss: 1.37091e-06 -> 1.13249e-06 diff=2.03e-06,2.03e-06 Linf=2.980e-07
working on output/mnist/3860.model with loss=3.33786e-06
#0: loss: 3.33786e-06 -> 2.86102e-06 diff=4.77e-07,4.77e-07 Linf=2.086e-07
#0: loss: 2.86102e-06 -> 1.78814e-06 diff=1.55e-06,1.55e-06 Linf=2.086e-07
#10: loss: 1.78814e-06 -> 1.66893e-06 diff=1.67e-06,1.67e-06 Linf=2.086e-07
#10: loss: 1.66893e-06 -> 1.43051e-06 diff=1.91e-06,1.91e-06 Linf=2.086e-07
#341: loss: 1.43051e-06 -> 1.31130e-06 diff=2.03e-06,2.03e-06 Linf=2.086e-07
#348: loss: 1.31130e-06 -> 1.19209e-06 diff=2.15e-06,2.15e-06 Linf=3.311e-07
working on output/mnist/1461.model with loss=3.5762787e-06
#0: loss: 3.57628e-06 -> 3.33786e-06 diff=2.38e-07,2.38e-07 Linf=1.791e-07
#2: loss: 3.33786e-06 -> 3.21865e-06 diff=3.58e-07,3.58e-07 Linf=2.384e-07
#2: loss: 3.21865e-06 -> 3.09944e-06 diff=4.77e-07,4.77e-07 Linf=2.384e-07
#3: loss: 3.09944e-06 -> 2.98023e-06 diff=5.96e-07,5.96e-07 Linf=2.384e-07
#4: loss: 2.98023e-06 -> 2.62260e-06 diff=9.54e-07,9.54e-07 Linf=3.451e-07
#4: loss: 2.62260e-06 -> 2.50340e-06 diff=1.07e-06,1.07e-06 Linf=3.451e-07
#15: loss: 2.50340e-06 -> 2.38419e-06 diff=1.19e-06,1.19e-06 Linf=3.451e-07
#15: loss: 2.38419e-06 -> 2.14577e-06 diff=1.43e-06,1.43e-06 Linf=3.451e-07
#15: loss: 2.14577e-06 -> 1.90735e-06 diff=1.67e-06,1.67e-06 Linf=3.837e-07
#492: loss: 1.90735e-06 -> 1.78814e-06 diff=1.79e-06,1.79e-06 Linf=3.837e-07
#592: loss: 1.78814e-06 -> 1.66893e-06 diff=1.91e-06,1.91e-06 Linf=3.837e-07
#722: loss: 1.66893e-06 -> 1.54972e-06 diff=2.03e-06,2.03e-06 Linf=3.837e-07
#724: loss: 1.54972e-06 -> 1.43051e-06 diff=2.15e-06,2.15e-06 Linf=3.837e-07
#724: loss: 1.43051e-06 -> 1.31130e-06 diff=2.26e-06,2.26e-06 Linf=3.837e-07
working on output/mnist/2782.model with loss=4.5597553e-06
#0: loss: 4.55976e-06 -> 4.32134e-06 diff=2.38e-07,2.38e-07 Linf=1.675e-07
#0: loss: 4.32134e-06 -> 3.96371e-06 diff=5.96e-07,5.96e-07 Linf=2.384e-07
#0: loss: 3.96371e-06 -> 3.66569e-06 diff=8.94e-07,8.94e-07 Linf=2.384e-07
#0: loss: 3.66569e-06 -> 3.48687e-06 diff=1.07e-06,1.07e-06 Linf=2.384e-07
#2: loss: 3.48687e-06 -> 3.36766e-06 diff=1.19e-06,1.19e-06 Linf=2.384e-07
#3: loss: 3.36766e-06 -> 3.24845e-06 diff=1.31e-06,1.31e-06 Linf=3.561e-07
#3: loss: 3.24845e-06 -> 3.15905e-06 diff=1.40e-06,1.40e-06 Linf=3.561e-07
#5: loss: 3.15905e-06 -> 3.12924e-06 diff=1.43e-06,1.43e-06 Linf=3.561e-07
#5: loss: 3.12924e-06 -> 2.89083e-06 diff=1.67e-06,1.67e-06 Linf=3.561e-07
#7: loss: 2.89083e-06 -> 2.83122e-06 diff=1.73e-06,1.73e-06 Linf=3.561e-07
#20: loss: 2.83122e-06 -> 2.77162e-06 diff=1.79e-06,1.79e-06 Linf=3.561e-07
#126: loss: 2.77162e-06 -> 2.71201e-06 diff=1.85e-06,1.85e-06 Linf=3.561e-07
#150: loss: 2.71201e-06 -> 2.65241e-06 diff=1.91e-06,1.91e-06 Linf=3.561e-07
#183: loss: 2.65241e-06 -> 2.62260e-06 diff=1.94e-06,1.94e-06 Linf=3.561e-07
#190: loss: 2.62260e-06 -> 2.50340e-06 diff=2.06e-06,2.06e-06 Linf=3.561e-07
#244: loss: 2.50340e-06 -> 2.41399e-06 diff=2.15e-06,2.15e-06 Linf=3.561e-07
#244: loss: 2.41399e-06 -> 2.29478e-06 diff=2.26e-06,2.26e-06 Linf=3.561e-07
#263: loss: 2.29478e-06 -> 2.26498e-06 diff=2.29e-06,2.29e-06 Linf=3.561e-07
#333: loss: 2.26498e-06 -> 2.23517e-06 diff=2.32e-06,2.32e-06 Linf=3.561e-07
working on output/mnist/1212.model with loss=4.708767e-06
#0: loss: 4.70877e-06 -> 4.58956e-06 diff=1.19e-07,1.19e-07 Linf=2.384e-07
#0: loss: 4.58956e-06 -> 4.47035e-06 diff=2.38e-07,2.38e-07 Linf=2.384e-07
#0: loss: 4.47035e-06 -> 4.23193e-06 diff=4.77e-07,4.77e-07 Linf=2.384e-07
#1: loss: 4.23193e-06 -> 3.75509e-06 diff=9.54e-07,9.54e-07 Linf=2.980e-07
#2: loss: 3.75509e-06 -> 3.63588e-06 diff=1.07e-06,1.07e-06 Linf=2.980e-07
#3: loss: 3.63588e-06 -> 3.51667e-06 diff=1.19e-06,1.19e-06 Linf=3.614e-07
#6: loss: 3.51667e-06 -> 3.39746e-06 diff=1.31e-06,1.31e-06 Linf=3.614e-07
#11: loss: 3.39746e-06 -> 3.27826e-06 diff=1.43e-06,1.43e-06 Linf=3.614e-07
#89: loss: 3.27826e-06 -> 3.15905e-06 diff=1.55e-06,1.55e-06 Linf=3.614e-07
