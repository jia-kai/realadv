preprocessing on output/mnist/1212.model
adv_loss=4.947185516357422e-06 adv_loss_mm=4.410743713378906e-06 inp_loss=4.273809432983398 gap=-1.518417445762958
preprocessing on output/mnist/1459.model
adv_loss=3.0994415283203125e-06 adv_loss_mm=3.159046173095703e-06 inp_loss=4.129624366760254 gap=-1.0982149079709458
preprocessing on output/mnist/1461.model
adv_loss=3.337860107421875e-06 adv_loss_mm=3.993511199951172e-06 inp_loss=3.3484535217285156 gap=-2.074641341689596
preprocessing on output/mnist/2641.model
adv_loss=1.4603137969970703e-06 adv_loss_mm=2.294778823852539e-06 inp_loss=3.1373472213745117 gap=-2.2001739081808616
preprocessing on output/mnist/2782.model
adv_loss=4.172325134277344e-06 adv_loss_mm=3.039836883544922e-06 inp_loss=3.4540233612060547 gap=-0.03957562719990185
preprocessing on output/mnist/3860.model
adv_loss=9.5367431640625e-07 adv_loss_mm=1.5497207641601562e-06 inp_loss=3.865035057067871 gap=-3.673972438462156
preprocessing on output/mnist/3966.model
adv_loss=1.6987323760986328e-06 adv_loss_mm=1.6391277313232422e-06 inp_loss=2.7201290130615234 gap=-5.0190233333916646e-08
preprocessing on output/mnist/41.model
adv_loss=2.0265579223632812e-06 adv_loss_mm=2.384185791015625e-06 inp_loss=2.5272514820098877 gap=-1.925067271937266e-07
preprocessing on output/mnist/4798.model
adv_loss=1.7285346984863281e-06 adv_loss_mm=2.2649765014648438e-06 inp_loss=3.202498197555542 gap=-0.8083748030881506
preprocessing on output/mnist/6327.model
adv_loss=1.6987323760986328e-06 adv_loss_mm=1.5795230865478516e-06 inp_loss=3.296889305114746 gap=-2.081276688754929e-07
preprocessing on output/mnist/6576.model
adv_loss=2.7418136596679688e-06 adv_loss_mm=1.6689300537109375e-06 inp_loss=2.9622538089752197 gap=-1.6094605991540831
preprocessing on output/mnist/6791.model
adv_loss=1.5050172805786133e-06 adv_loss_mm=2.1904706954956055e-06 inp_loss=3.855414628982544 gap=-0.09270705339313404
preprocessing on output/mnist/6929.model
adv_loss=1.1771917343139648e-06 adv_loss_mm=1.5944242477416992e-06 inp_loss=4.787815093994141 gap=-1.845350300828507
preprocessing on output/mnist/7186.model
adv_loss=4.76837158203125e-07 adv_loss_mm=1.5497207641601562e-06 inp_loss=3.5544910430908203 gap=-7.865661580616019e-07
preprocessing on output/mnist/850.model
adv_loss=1.6689300537109375e-06 adv_loss_mm=2.1457672119140625e-06 inp_loss=3.725200653076172 gap=-2.680255589217079e-06
preprocessing on output/mnist/8571.model
adv_loss=3.5762786865234375e-06 adv_loss_mm=2.6226043701171875e-06 inp_loss=3.511122941970825 gap=-7.943013895155949e-07
preprocessing on output/mnist/890.model
adv_loss=8.344650268554688e-07 adv_loss_mm=1.5497207641601562e-06 inp_loss=2.5032894611358643 gap=-1.022442080828036e-07
preprocessing on output/mnist/9285.model
adv_loss=1.9073486328125e-06 adv_loss_mm=3.2186508178710938e-06 inp_loss=2.972046375274658 gap=-0.48362446971144474
working on output/mnist/7186.model with loss=4.7683716e-07
#0: loss: 4.76837e-07 -> 1.19209e-07 diff=3.58e-07,1.43e-06 Linf=1.788e-07
#2: loss: 1.19209e-07 -> -4.76837e-07 diff=9.54e-07,2.03e-06 Linf=1.788e-07
#3: loss: -4.76837e-07 -> -5.96046e-07 diff=1.07e-06,2.15e-06 Linf=1.788e-07
#5: loss: -5.96046e-07 -> -7.15256e-07 diff=1.19e-06,2.26e-06 Linf=1.882e-07
#10: loss: -7.15256e-07 -> -8.34465e-07 diff=1.31e-06,2.38e-06 Linf=2.973e-07
#201: loss: -8.34465e-07 -> -9.53674e-07 diff=1.43e-06,2.50e-06 Linf=2.973e-07
conv [-8.2  -1.35 -5.06 -2.56 -4.39 -1.35 -5.48 -5.48 -2.65 -3.  ] l=5 cw=-9.54e-07
mm   [-8.2  -1.35 -5.06 -2.56 -4.39 -1.35 -5.48 -5.48 -2.65 -3.  ] l=1 cw=2.03e-06
test acc: 98.57%
working on output/mnist/890.model with loss=8.34465e-07
#0: loss: 8.34465e-07 -> 5.96046e-07 diff=2.38e-07,9.54e-07 Linf=1.788e-07
#5: loss: 5.96046e-07 -> 4.76837e-07 diff=3.58e-07,1.07e-06 Linf=2.384e-07
#174: loss: 4.76837e-07 -> 2.38419e-07 diff=5.96e-07,1.31e-06 Linf=3.427e-07
#182: loss: 2.38419e-07 -> 1.19209e-07 diff=7.15e-07,1.43e-06 Linf=3.427e-07
working on output/mnist/3860.model with loss=9.536743e-07
#0: loss: 9.53674e-07 -> 7.15256e-07 diff=2.38e-07,8.34e-07 Linf=1.490e-07
#2: loss: 7.15256e-07 -> 5.96046e-07 diff=3.58e-07,9.54e-07 Linf=2.384e-07
#3: loss: 5.96046e-07 -> 2.38419e-07 diff=7.15e-07,1.31e-06 Linf=3.117e-07
#5: loss: 2.38419e-07 -> -2.38419e-07 diff=1.19e-06,1.79e-06 Linf=3.117e-07
#20: loss: -2.38419e-07 -> -3.57628e-07 diff=1.31e-06,1.91e-06 Linf=3.117e-07
#127: loss: -3.57628e-07 -> -8.34465e-07 diff=1.79e-06,2.38e-06 Linf=3.117e-07
conv [-8.86 -5.92 -1.9   0.88 -7.46 -1.99 -9.83  0.88 -1.39 -3.25] l=7 cw=-8.34e-07
mm   [-8.86 -5.92 -1.9   0.88 -7.46 -1.99 -9.83  0.88 -1.39 -3.25] l=3 cw=2.74e-06
test acc: 97.83%
working on output/mnist/6929.model with loss=1.1771917e-06
#0: loss: 1.17719e-06 -> 1.05798e-06 diff=1.19e-07,5.36e-07 Linf=1.565e-07
#2: loss: 1.05798e-06 -> 9.38773e-07 diff=2.38e-07,6.56e-07 Linf=2.384e-07
#2: loss: 9.38773e-07 -> 8.79169e-07 diff=2.98e-07,7.15e-07 Linf=2.384e-07
#3: loss: 8.79169e-07 -> 8.19564e-07 diff=3.58e-07,7.75e-07 Linf=2.384e-07
#3: loss: 8.19564e-07 -> 5.81145e-07 diff=5.96e-07,1.01e-06 Linf=2.384e-07
#5: loss: 5.81145e-07 -> 5.21541e-07 diff=6.56e-07,1.07e-06 Linf=2.384e-07
#8: loss: 5.21541e-07 -> 4.61936e-07 diff=7.15e-07,1.13e-06 Linf=3.576e-07
#13: loss: 4.61936e-07 -> 2.23517e-07 diff=9.54e-07,1.37e-06 Linf=3.576e-07
#14: loss: 2.23517e-07 -> 1.63913e-07 diff=1.01e-06,1.43e-06 Linf=3.576e-07
#506: loss: 1.63913e-07 -> -7.45058e-08 diff=1.25e-06,1.67e-06 Linf=3.576e-07
#592: loss: -7.45058e-08 -> -1.34110e-07 diff=1.31e-06,1.73e-06 Linf=3.576e-07
#594: loss: -1.34110e-07 -> -2.53320e-07 diff=1.43e-06,1.85e-06 Linf=3.576e-07
#981: loss: -2.53320e-07 -> -3.72529e-07 diff=1.55e-06,1.97e-06 Linf=3.576e-07
conv [-5.97 -6.51 -5.8  -5.91  0.19 -6.92 -4.68 -5.63  0.19 -3.06] l=8 cw=-3.73e-07
mm   [-5.97 -6.51 -5.8  -5.91  0.19 -6.92 -4.68 -5.63  0.19 -3.06] l=4 cw=2.31e-06
test acc: 98.14%
verification of original model failed: {'ObjectiveBound': -1.012973834979558e-06,
 'ObjectiveValue': -1.012973834979558e-06,
 'PredictedIndex': 5,
 'SolveStatus': 'Optimal',
 'SolveTime': 21.228271007537842,
 'TargetIndexes': array([ 1,  2,  3,  4,  6,  7,  8,  9, 10]),
 'TighteningApproach': array([[108],
       [112]], dtype=uint16),
 'TotalTime': 30.623545468,
 'robust': False,
 'status_known': True}
retrying with new gap -1.8453497886657715 ...
conv [-5.97 -6.51 -5.8  -5.91  0.19 -6.92 -4.68 -5.63  0.19 -3.06] l=4 cw=1.04e-07
mm   [-5.97 -6.51 -5.8  -5.91  0.19 -6.92 -4.68 -5.63  0.19 -3.06] l=4 cw=2.79e-06
test acc: 98.14%
working on output/mnist/2641.model with loss=1.4603138e-06
#0: loss: 1.46031e-06 -> 1.22190e-06 diff=2.38e-07,1.07e-06 Linf=1.793e-07
#0: loss: 1.22190e-06 -> 1.04308e-06 diff=4.17e-07,1.25e-06 Linf=2.384e-07
#0: loss: 1.04308e-06 -> 8.04663e-07 diff=6.56e-07,1.49e-06 Linf=2.384e-07
#2: loss: 8.04663e-07 -> 7.45058e-07 diff=7.15e-07,1.55e-06 Linf=2.384e-07
#3: loss: 7.45058e-07 -> 6.85453e-07 diff=7.75e-07,1.61e-06 Linf=2.840e-07
#3: loss: 6.85453e-07 -> 5.66244e-07 diff=8.94e-07,1.73e-06 Linf=2.840e-07
#7: loss: 5.66244e-07 -> 5.06639e-07 diff=9.54e-07,1.79e-06 Linf=2.870e-07
#39: loss: 5.06639e-07 -> 4.47035e-07 diff=1.01e-06,1.85e-06 Linf=2.737e-07
#48: loss: 4.47035e-07 -> 3.27826e-07 diff=1.13e-06,1.97e-06 Linf=2.737e-07
#150: loss: 3.27826e-07 -> 1.49012e-07 diff=1.31e-06,2.15e-06 Linf=2.737e-07
#379: loss: 1.49012e-07 -> 8.94070e-08 diff=1.37e-06,2.21e-06 Linf=2.980e-07
#379: loss: 8.94070e-08 -> -2.98023e-08 diff=1.49e-06,2.32e-06 Linf=2.980e-07
conv [-6.83 -4.77 -4.53 -5.57 -3.72 -0.23 -0.23 -7.11 -1.89 -5.37] l=5 cw=-2.98e-08
mm   [-6.83 -4.77 -4.53 -5.57 -3.72 -0.23 -0.23 -7.11 -1.89 -5.37] l=6 cw=2.53e-06
test acc: 98.40%
working on output/mnist/6791.model with loss=1.5050173e-06
#0: loss: 1.50502e-06 -> 1.26660e-06 diff=2.38e-07,9.24e-07 Linf=1.445e-07
#0: loss: 1.26660e-06 -> 1.02818e-06 diff=4.77e-07,1.16e-06 Linf=1.863e-07
#6: loss: 1.02818e-06 -> 9.68575e-07 diff=5.36e-07,1.22e-06 Linf=1.863e-07
working on output/mnist/850.model with loss=1.66893e-06
#0: loss: 1.66893e-06 -> 1.43051e-06 diff=2.38e-07,7.15e-07 Linf=1.784e-07
#2: loss: 1.43051e-06 -> 1.31130e-06 diff=3.58e-07,8.34e-07 Linf=2.384e-07
#3: loss: 1.31130e-06 -> 1.07288e-06 diff=5.96e-07,1.07e-06 Linf=2.384e-07
#5: loss: 1.07288e-06 -> 9.53674e-07 diff=7.15e-07,1.19e-06 Linf=2.384e-07
#5: loss: 9.53674e-07 -> 8.34465e-07 diff=8.34e-07,1.31e-06 Linf=2.384e-07
#7: loss: 8.34465e-07 -> 7.15256e-07 diff=9.54e-07,1.43e-06 Linf=2.384e-07
#25: loss: 7.15256e-07 -> 5.96046e-07 diff=1.07e-06,1.55e-06 Linf=2.384e-07
#55: loss: 5.96046e-07 -> 4.76837e-07 diff=1.19e-06,1.67e-06 Linf=2.384e-07
#96: loss: 4.76837e-07 -> 3.57628e-07 diff=1.31e-06,1.79e-06 Linf=2.384e-07
#262: loss: 3.57628e-07 -> 2.38419e-07 diff=1.43e-06,1.91e-06 Linf=2.583e-07
working on output/mnist/3966.model with loss=1.6987324e-06
#0: loss: 1.69873e-06 -> 1.57952e-06 diff=1.19e-07,5.96e-08 Linf=1.911e-07
#0: loss: 1.57952e-06 -> 1.25170e-06 diff=4.47e-07,3.87e-07 Linf=2.384e-07
#1: loss: 1.25170e-06 -> 1.19209e-06 diff=5.07e-07,4.47e-07 Linf=2.384e-07
#1: loss: 1.19209e-06 -> 1.16229e-06 diff=5.36e-07,4.77e-07 Linf=2.384e-07
#3: loss: 1.16229e-06 -> 1.13249e-06 diff=5.66e-07,5.07e-07 Linf=3.576e-07
#4: loss: 1.13249e-06 -> 1.01328e-06 diff=6.85e-07,6.26e-07 Linf=3.576e-07
#4: loss: 1.01328e-06 -> 7.74860e-07 diff=9.24e-07,8.64e-07 Linf=3.576e-07
#33: loss: 7.74860e-07 -> 5.66244e-07 diff=1.13e-06,1.07e-06 Linf=3.576e-07
#51: loss: 5.66244e-07 -> 5.36442e-07 diff=1.16e-06,1.10e-06 Linf=3.576e-07
#109: loss: 5.36442e-07 -> 4.76837e-07 diff=1.22e-06,1.16e-06 Linf=4.321e-07
#142: loss: 4.76837e-07 -> 4.47035e-07 diff=1.25e-06,1.19e-06 Linf=4.321e-07
#147: loss: 4.47035e-07 -> 4.17233e-07 diff=1.28e-06,1.22e-06 Linf=4.321e-07
#170: loss: 4.17233e-07 -> 3.57628e-07 diff=1.34e-06,1.28e-06 Linf=4.321e-07
#217: loss: 3.57628e-07 -> 2.98023e-07 diff=1.40e-06,1.34e-06 Linf=5.588e-07
#230: loss: 2.98023e-07 -> 2.38419e-07 diff=1.46e-06,1.40e-06 Linf=5.588e-07
#251: loss: 2.38419e-07 -> 1.78814e-07 diff=1.52e-06,1.46e-06 Linf=5.588e-07
#259: loss: 1.78814e-07 -> 1.19209e-07 diff=1.58e-06,1.52e-06 Linf=5.588e-07
working on output/mnist/6327.model with loss=1.6987324e-06
#0: loss: 1.69873e-06 -> 1.16229e-06 diff=5.36e-07,4.17e-07 Linf=1.788e-07
#1: loss: 1.16229e-06 -> 1.04308e-06 diff=6.56e-07,5.36e-07 Linf=1.788e-07
#2: loss: 1.04308e-06 -> 9.83477e-07 diff=7.15e-07,5.96e-07 Linf=2.086e-07
#3: loss: 9.83477e-07 -> 7.45058e-07 diff=9.54e-07,8.34e-07 Linf=2.086e-07
#4: loss: 7.45058e-07 -> 6.85453e-07 diff=1.01e-06,8.94e-07 Linf=2.086e-07
#4: loss: 6.85453e-07 -> 5.06639e-07 diff=1.19e-06,1.07e-06 Linf=2.086e-07
#6: loss: 5.06639e-07 -> 1.49012e-07 diff=1.55e-06,1.43e-06 Linf=2.086e-07
#235: loss: 1.49012e-07 -> 8.94070e-08 diff=1.61e-06,1.49e-06 Linf=3.278e-07
#661: loss: 8.94070e-08 -> 2.98023e-08 diff=1.67e-06,1.55e-06 Linf=3.278e-07
working on output/mnist/4798.model with loss=1.7285347e-06
#0: loss: 1.72853e-06 -> 1.60933e-06 diff=1.19e-07,6.56e-07 Linf=1.639e-07
#1: loss: 1.60933e-06 -> 1.54972e-06 diff=1.79e-07,7.15e-07 Linf=1.639e-07
#27: loss: 1.54972e-06 -> 1.37091e-06 diff=3.58e-07,8.94e-07 Linf=1.832e-07
#27: loss: 1.37091e-06 -> 1.25170e-06 diff=4.77e-07,1.01e-06 Linf=1.998e-07
working on output/mnist/9285.model with loss=1.9073486e-06
#2: loss: 1.90735e-06 -> 1.78814e-06 diff=1.19e-07,1.43e-06 Linf=1.788e-07
#3: loss: 1.78814e-06 -> 1.66893e-06 diff=2.38e-07,1.55e-06 Linf=1.788e-07
#4: loss: 1.66893e-06 -> 1.43051e-06 diff=4.77e-07,1.79e-06 Linf=1.800e-07
#9: loss: 1.43051e-06 -> 1.19209e-06 diff=7.15e-07,2.03e-06 Linf=2.012e-07
working on output/mnist/41.model with loss=2.026558e-06
#0: loss: 2.02656e-06 -> 1.96695e-06 diff=5.96e-08,4.17e-07 Linf=1.849e-07
#0: loss: 1.96695e-06 -> 1.84774e-06 diff=1.79e-07,5.36e-07 Linf=1.849e-07
#0: loss: 1.84774e-06 -> 1.54972e-06 diff=4.77e-07,8.34e-07 Linf=1.849e-07
#3: loss: 1.54972e-06 -> 1.49012e-06 diff=5.36e-07,8.94e-07 Linf=2.086e-07
#4: loss: 1.49012e-06 -> 1.31130e-06 diff=7.15e-07,1.07e-06 Linf=2.980e-07
#5: loss: 1.31130e-06 -> 1.19209e-06 diff=8.34e-07,1.19e-06 Linf=2.980e-07
#10: loss: 1.19209e-06 -> 9.53674e-07 diff=1.07e-06,1.43e-06 Linf=2.980e-07
#128: loss: 9.53674e-07 -> 8.34465e-07 diff=1.19e-06,1.55e-06 Linf=2.980e-07
#174: loss: 8.34465e-07 -> 7.74860e-07 diff=1.25e-06,1.61e-06 Linf=2.980e-07
#180: loss: 7.74860e-07 -> 6.55651e-07 diff=1.37e-06,1.73e-06 Linf=2.980e-07
working on output/mnist/6576.model with loss=2.7418137e-06
#0: loss: 2.74181e-06 -> 2.26498e-06 diff=4.77e-07,-5.96e-07 Linf=2.086e-07
#1: loss: 2.26498e-06 -> 1.90735e-06 diff=8.34e-07,-2.38e-07 Linf=2.086e-07
#1: loss: 1.90735e-06 -> 1.43051e-06 diff=1.31e-06,2.38e-07 Linf=2.086e-07
#2: loss: 1.43051e-06 -> 1.31130e-06 diff=1.43e-06,3.58e-07 Linf=2.086e-07
#78: loss: 1.31130e-06 -> 1.19209e-06 diff=1.55e-06,4.77e-07 Linf=2.086e-07
#80: loss: 1.19209e-06 -> 1.07288e-06 diff=1.67e-06,5.96e-07 Linf=2.086e-07
#81: loss: 1.07288e-06 -> 8.34465e-07 diff=1.91e-06,8.34e-07 Linf=2.086e-07
#299: loss: 8.34465e-07 -> 5.96046e-07 diff=2.15e-06,1.07e-06 Linf=2.086e-07
working on output/mnist/1459.model with loss=3.0994415e-06
#0: loss: 3.09944e-06 -> 2.86102e-06 diff=2.38e-07,2.98e-07 Linf=2.384e-07
#0: loss: 2.86102e-06 -> 2.68221e-06 diff=4.17e-07,4.77e-07 Linf=2.384e-07
#0: loss: 2.68221e-06 -> 2.44379e-06 diff=6.56e-07,7.15e-07 Linf=2.384e-07
#1: loss: 2.44379e-06 -> 2.38419e-06 diff=7.15e-07,7.75e-07 Linf=2.384e-07
#1: loss: 2.38419e-06 -> 2.14577e-06 diff=9.54e-07,1.01e-06 Linf=2.384e-07
#3: loss: 2.14577e-06 -> 2.02656e-06 diff=1.07e-06,1.13e-06 Linf=2.980e-07
#124: loss: 2.02656e-06 -> 1.90735e-06 diff=1.19e-06,1.25e-06 Linf=2.980e-07
#134: loss: 1.90735e-06 -> 1.78814e-06 diff=1.31e-06,1.37e-06 Linf=2.715e-07
#137: loss: 1.78814e-06 -> 1.66893e-06 diff=1.43e-06,1.49e-06 Linf=2.715e-07
#151: loss: 1.66893e-06 -> 1.54972e-06 diff=1.55e-06,1.61e-06 Linf=3.576e-07
#299: loss: 1.54972e-06 -> 1.43051e-06 diff=1.67e-06,1.73e-06 Linf=3.576e-07
#461: loss: 1.43051e-06 -> 1.31130e-06 diff=1.79e-06,1.85e-06 Linf=3.576e-07
working on output/mnist/1461.model with loss=3.33786e-06
#0: loss: 3.33786e-06 -> 3.21865e-06 diff=1.19e-07,7.75e-07 Linf=1.937e-07
#0: loss: 3.21865e-06 -> 3.09944e-06 diff=2.38e-07,8.94e-07 Linf=2.384e-07
#0: loss: 3.09944e-06 -> 2.74181e-06 diff=5.96e-07,1.25e-06 Linf=2.384e-07
#4: loss: 2.74181e-06 -> 2.62260e-06 diff=7.15e-07,1.37e-06 Linf=2.384e-07
#131: loss: 2.62260e-06 -> 2.56300e-06 diff=7.75e-07,1.43e-06 Linf=3.264e-07
#431: loss: 2.56300e-06 -> 2.50340e-06 diff=8.34e-07,1.49e-06 Linf=3.264e-07
#566: loss: 2.50340e-06 -> 2.44379e-06 diff=8.94e-07,1.55e-06 Linf=3.264e-07
#586: loss: 2.44379e-06 -> 2.38419e-06 diff=9.54e-07,1.61e-06 Linf=4.562e-07
working on output/mnist/8571.model with loss=3.5762787e-06
#0: loss: 3.57628e-06 -> 2.86102e-06 diff=7.15e-07,-2.38e-07 Linf=1.788e-07
#1: loss: 2.86102e-06 -> 2.38419e-06 diff=1.19e-06,2.38e-07 Linf=1.788e-07
#2: loss: 2.38419e-06 -> 1.90735e-06 diff=1.67e-06,7.15e-07 Linf=1.982e-07
#79: loss: 1.90735e-06 -> 1.43051e-06 diff=2.15e-06,1.19e-06 Linf=2.012e-07
#137: loss: 1.43051e-06 -> 1.19209e-06 diff=2.38e-06,1.43e-06 Linf=2.012e-07
#229: loss: 1.19209e-06 -> 9.53674e-07 diff=2.62e-06,1.67e-06 Linf=2.012e-07
working on output/mnist/2782.model with loss=4.172325e-06
#0: loss: 4.17233e-06 -> 4.14252e-06 diff=2.98e-08,-1.10e-06 Linf=2.384e-07
#0: loss: 4.14252e-06 -> 3.90410e-06 diff=2.68e-07,-8.64e-07 Linf=2.384e-07
#0: loss: 3.90410e-06 -> 3.54648e-06 diff=6.26e-07,-5.07e-07 Linf=2.384e-07
#0: loss: 3.54648e-06 -> 3.42727e-06 diff=7.45e-07,-3.87e-07 Linf=2.384e-07
#0: loss: 3.42727e-06 -> 3.33786e-06 diff=8.34e-07,-2.98e-07 Linf=2.384e-07
#0: loss: 3.33786e-06 -> 3.15905e-06 diff=1.01e-06,-1.19e-07 Linf=2.384e-07
#1: loss: 3.15905e-06 -> 3.03984e-06 diff=1.13e-06,0.00e+00 Linf=2.384e-07
#5: loss: 3.03984e-06 -> 2.92063e-06 diff=1.25e-06,1.19e-07 Linf=3.278e-07
#13: loss: 2.92063e-06 -> 2.74181e-06 diff=1.43e-06,2.98e-07 Linf=3.278e-07
#18: loss: 2.74181e-06 -> 2.62260e-06 diff=1.55e-06,4.17e-07 Linf=3.278e-07
#203: loss: 2.62260e-06 -> 2.56300e-06 diff=1.61e-06,4.77e-07 Linf=3.278e-07
#341: loss: 2.56300e-06 -> 2.50340e-06 diff=1.67e-06,5.36e-07 Linf=3.576e-07
#343: loss: 2.50340e-06 -> 2.38419e-06 diff=1.79e-06,6.56e-07 Linf=3.576e-07
#343: loss: 2.38419e-06 -> 2.26498e-06 diff=1.91e-06,7.75e-07 Linf=3.576e-07
#344: loss: 2.26498e-06 -> 2.14577e-06 diff=2.03e-06,8.94e-07 Linf=3.576e-07
#410: loss: 2.14577e-06 -> 2.02656e-06 diff=2.15e-06,1.01e-06 Linf=3.576e-07
#440: loss: 2.02656e-06 -> 1.96695e-06 diff=2.21e-06,1.07e-06 Linf=3.576e-07
#442: loss: 1.96695e-06 -> 1.84774e-06 diff=2.32e-06,1.19e-06 Linf=3.789e-07
#449: loss: 1.84774e-06 -> 1.72853e-06 diff=2.44e-06,1.31e-06 Linf=3.789e-07
#460: loss: 1.72853e-06 -> 1.63913e-06 diff=2.53e-06,1.40e-06 Linf=3.789e-07
working on output/mnist/1212.model with loss=4.9471855e-06
#0: loss: 4.94719e-06 -> 4.58956e-06 diff=3.58e-07,-1.79e-07 Linf=2.012e-07
#0: loss: 4.58956e-06 -> 4.47035e-06 diff=4.77e-07,-5.96e-08 Linf=2.012e-07
#1: loss: 4.47035e-06 -> 4.23193e-06 diff=7.15e-07,1.79e-07 Linf=3.320e-07
#1: loss: 4.23193e-06 -> 3.75509e-06 diff=1.19e-06,6.56e-07 Linf=3.320e-07
#3: loss: 3.75509e-06 -> 3.69549e-06 diff=1.25e-06,7.15e-07 Linf=3.320e-07
#19: loss: 3.69549e-06 -> 3.51667e-06 diff=1.43e-06,8.94e-07 Linf=3.320e-07
#20: loss: 3.51667e-06 -> 3.39746e-06 diff=1.55e-06,1.01e-06 Linf=3.320e-07
#21: loss: 3.39746e-06 -> 3.33786e-06 diff=1.61e-06,1.07e-06 Linf=3.320e-07
#49: loss: 3.33786e-06 -> 3.27826e-06 diff=1.67e-06,1.13e-06 Linf=3.320e-07
#51: loss: 3.27826e-06 -> 3.15905e-06 diff=1.79e-06,1.25e-06 Linf=3.320e-07
#193: loss: 3.15905e-06 -> 3.09944e-06 diff=1.85e-06,1.31e-06 Linf=3.320e-07
#198: loss: 3.09944e-06 -> 2.92063e-06 diff=2.03e-06,1.49e-06 Linf=3.320e-07
#211: loss: 2.92063e-06 -> 2.86102e-06 diff=2.09e-06,1.55e-06 Linf=3.585e-07
